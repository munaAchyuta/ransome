## pre work views useful for further work (specifically on word2vec with crf for named entity recognition or ner)

Wang and Manning (2013) showed that linear architectures perform better in high-dimensional discrete feature space than non-linear ones,
whereas non-linear architectures are more effec-tive in low-dimensional and continuous feature space. Hence, the previous method that directly
uses the continuous word embeddings as features in linear models (CRF) is inappropriate. Word embeddings may be better utilized in the linear modeling framework by smartly transforming the
embeddings to some relatively higher dimensional and discrete representations.

* http://ir.hit.edu.cn/~jguo/papers/emnlp2014-semiemb.pdf
* http://cogcomp.org/files/presentations/BGU_WordRepresentations_2009.pdf
* https://graphaware.com/nlp/2018/09/10/deep-text-understand-combining-graphs-ner-word2vec.html
* http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/
* NER with less data -- https://arxiv.org/pdf/1806.04411.pdf
* FREME NER TOOL -- https://freme-project.github.io/knowledge-base/freme-for-api-users/freme-ner.html
* crf+cluster  vector+dictionary+embeddings for disease NER -- http://www.dialog-21.ru/media/3932/miftahutdinovzshetal.pdf
* ultra fine entity typing.(http://nlp.cs.washington.edu/entity_type/slides.pdf)(https://github.com/uwnlp/open_type#eusol-choi-omer-levy-yejin-choi-and-luke-zettlemoyer-acl-2018)
* Fine-Grained Entity Typing in Hyperbolic Space (https://github.com/nlpAThits/figet-hyperbolic-space)
* Fine-grained Entity Typing through Increased Discourse Contextand Adaptive Classification Thresholds (https://arxiv.org/pdf/1804.08000v1.pdf)
