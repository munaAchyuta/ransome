## pre work views useful for further work (specifically on word2vec with crf for named entity recognition or ner)

Wang and Manning (2013) showed that linear architectures perform better in high-dimensional discrete feature space than non-linear ones,
whereas non-linear architectures are more effec-tive in low-dimensional and continuous feature space. Hence, the previous method that directly
uses the continuous word embeddings as features in linear models (CRF) is inappropriate. Word embeddings may be better utilized in the linear modeling framework by smartly transforming the
embeddings to some relatively higher dimensional and discrete representations.

* http://ir.hit.edu.cn/~jguo/papers/emnlp2014-semiemb.pdf
* http://cogcomp.org/files/presentations/BGU_WordRepresentations_2009.pdf
* https://graphaware.com/nlp/2018/09/10/deep-text-understand-combining-graphs-ner-word2vec.html
* http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/
* NER with less data -- https://arxiv.org/pdf/1806.04411.pdf
* FREME NER TOOL -- https://freme-project.github.io/knowledge-base/freme-for-api-users/freme-ner.html
